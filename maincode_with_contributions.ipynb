{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7cdfa0",
   "metadata": {},
   "source": [
    "# Enhancing Student Success Forecasting Through Optimized Ensemble Learning Techniques\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this notebook, we implement the methodology described in our research paper to move from basic predictive modeling to advanced ensemble learning methods.\n",
    "\n",
    "Our main goals are to:\n",
    "\n",
    "- Predict academic outcomes (Pass/Fail) based on socio-demographic and school-related features.\n",
    "- Compare performance across different machine learning algorithms.\n",
    "- Identify impactful factors (such as family stability and home environment) that affect student achievement.\n",
    "- Determine the most robust algorithm with the highest accuracy for educational forecasting.\n",
    "\n",
    "We will be utilizing the following learning algorithms:\n",
    "\n",
    "- Logistic Regression \n",
    "- Support Vector Machine (SVM) \n",
    "- K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8fffc9",
   "metadata": {},
   "source": [
    "## Data Characterization and Target Distribution\n",
    "\n",
    "- Load the dataset and inspect its structure.\n",
    "- Check the class distribution of the target variable (`passed`).\n",
    "- Understand baseline accuracy (majority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720b355",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0e20a",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Establishes the dataset baseline and confirms the presence of moderate class imbalance.\n",
    "- Defines the Zero-R benchmark accuracy that predictive models must surpass.\n",
    "- Ensures that model evaluation is interpreted in a realistic classification context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from astropy.table import Table\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "df = pd.read_csv('student-data.csv')\n",
    "dfv = pd.read_csv('student-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122cddb",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Convert categorical columns into numerical format.\n",
    "- Apply feature scaling to normalize feature ranges.\n",
    "- Prepare the dataset for ML training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ca4ba",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping strings to numeric values:\n",
    "def numerical_data():\n",
    "    df['school'] = df['school'].map({'GP': 0, 'MS': 1})\n",
    "    df['sex'] = df['sex'].map({'M': 0, 'F': 1})\n",
    "    df['address'] = df['address'].map({'U': 0, 'R': 1})\n",
    "    df['famsize'] = df['famsize'].map({'LE3': 0, 'GT3': 1})\n",
    "    df['Pstatus'] = df['Pstatus'].map({'T': 0, 'A': 1})\n",
    "    df['Mjob'] = df['Mjob'].map({'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4})\n",
    "    df['Fjob'] = df['Fjob'].map({'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4})\n",
    "    df['reason'] = df['reason'].map({'home': 0, 'reputation': 1, 'course': 2, 'other': 3})\n",
    "    df['guardian'] = df['guardian'].map({'mother': 0, 'father': 1, 'other': 2})\n",
    "    df['schoolsup'] = df['schoolsup'].map({'no': 0, 'yes': 1})\n",
    "    df['famsup'] = df['famsup'].map({'no': 0, 'yes': 1})\n",
    "    df['paid'] = df['paid'].map({'no': 0, 'yes': 1})\n",
    "    df['activities'] = df['activities'].map({'no': 0, 'yes': 1})\n",
    "    df['nursery'] = df['nursery'].map({'no': 0, 'yes': 1})\n",
    "    df['higher'] = df['higher'].map({'no': 0, 'yes': 1})\n",
    "    df['internet'] = df['internet'].map({'no': 0, 'yes': 1})\n",
    "    df['romantic'] = df['romantic'].map({'no': 0, 'yes' : 1})\n",
    "    df['passed'] = df['passed'].map({'no': 0, 'yes': 1})\n",
    "    # reorder dataframe columns :\n",
    "    col = df['passed']\n",
    "    del df['passed']\n",
    "    df['passed'] = col\n",
    "\n",
    "    \n",
    "# feature scaling will allow the algorithm to converge faster, large data will have same scal\n",
    "def feature_scaling(df):\n",
    "    for i in df:\n",
    "        col = df[i]\n",
    "        # let's choose columns that have large values\n",
    "        if(np.max(col)>6):\n",
    "            Max = max(col)\n",
    "            Min = min(col)\n",
    "            mean = np.mean(col)\n",
    "            col  = (col-mean)/(Max)\n",
    "            df[i] = col\n",
    "        elif(np.max(col)<6):\n",
    "            col = (col-np.min(col))\n",
    "            col /= np.max(col)\n",
    "            df[i] = col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03e4c1",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Convert categorical columns into numerical format.\n",
    "- Apply feature scaling to normalize feature ranges.\n",
    "- Prepare the dataset for ML training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009982d",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All values in numerical after calling numerical_data() function\n",
    "numerical_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b5e4b",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "- Convert categorical columns into numerical format.\n",
    "- Apply feature scaling to normalize feature ranges.\n",
    "- Prepare the dataset for ML training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72e7b3",
   "metadata": {},
   "source": [
    "### Contribution (Aviral Bharadwaj)\n",
    "- Assisted in visualization and interpretation of model results.\n",
    "- Contributed to debugging and refining the notebook flow.\n",
    "- Helped in preparing explanations and documentation for this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scal our features\n",
    "feature_scaling(df)\n",
    "\n",
    "# Now we are ready for models training\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().shape # their is no null value \"fortunately:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
    "       'Walc', 'health', 'absences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of student status\n",
    "dfv['passed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'student pass the final exam ', 'student fail the final exam'\n",
    "sizes = [265, 130]\n",
    "colors=['lightskyblue','yellow']\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes,  labels=labels, autopct='%1.1f%%',colors=colors,\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60a0fc",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "- Compute Pearson correlation between all variables.\n",
    "- Identify strongly related features (e.g., parental education).\n",
    "- Detect negative indicators like previous failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1e9ef",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a4b4e",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Identifies statistically meaningful feature relationships that guide model interpretability.\n",
    "- Highlights critical positive clusters such as parental education synergy.\n",
    "- Detects strong negative indicators such as past failures, supporting early warning feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see correlation between variables through a correlation heatmap\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(corr, annot=True, cmap=\"Reds\")\n",
    "plt.title('Correlation Heatmap', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155161ed",
   "metadata": {},
   "source": [
    "## Feature Correlation Analysis\n",
    "\n",
    "- Compute Pearson correlation between all variables.\n",
    "- Identify strongly related features (e.g., parental education).\n",
    "- Detect negative indicators like previous failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae40e5",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311ee86",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Identifies statistically meaningful feature relationships that guide model interpretability.\n",
    "- Highlights critical positive clusters such as parental education synergy.\n",
    "- Detects strong negative indicators such as past failures, supporting early warning feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(df.corr()[['passed']].sort_values(by='passed', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with the status of student', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"goout\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b03ca7",
   "metadata": {},
   "source": [
    "## Behavioral and Demographic Insights\n",
    "\n",
    "- Analyze student social activity (`goout`) impact on success.\n",
    "- High social frequency often correlates with higher failure rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcaab1",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152cecc",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Provides evidence that behavioral attributes (e.g., social frequency) influence academic outcomes.\n",
    "- Supports the hypothesis that lifestyle factors contribute to student performance beyond grades.\n",
    "- Strengthens the justification for including socio-behavioral variables in predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# going out\n",
    "perc = (lambda col: col/col.sum())\n",
    "index = [0,1]\n",
    "out_tab = pd.crosstab(index=df.passed, columns=df.goout)\n",
    "out_perc = out_tab.apply(perc).reindex(index)\n",
    "out_perc.plot.bar(colormap=\"mako_r\", fontsize=16, figsize=(14,6))\n",
    "plt.title('student status  By Frequency of Going Out', fontsize=20)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)\n",
    "plt.xlabel('Student status', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2a08a",
   "metadata": {},
   "source": [
    "### Romantic Relationship Analysis\n",
    "- Compare pass/fail outcomes between students with and without romantic relationships.\n",
    "- Helps identify lifestyle distractions affecting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# romantic status\n",
    "romance_tab1 = pd.crosstab(index=df.passed, columns=df.romantic)\n",
    "romance_tab = np.log(romance_tab1)\n",
    "romance_perc = romance_tab.apply(perc).reindex(index)\n",
    "plt.figure()\n",
    "romance_perc.plot.bar(colormap=\"PiYG_r\", fontsize=16, figsize=(8,8))\n",
    "plt.title('Student status By Romantic relaion', fontsize=20)\n",
    "plt.ylabel('Percentage of Logarithm Student Counts ', fontsize=16)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.show()\n",
    "# 0 in romantic mean no romantic relation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e044d30",
   "metadata": {},
   "source": [
    "## Socio-Demographic Deep Dive\n",
    "This section explores how family background and parental occupation/education affect student outcomes.\n",
    "\n",
    "### Parental Influence and Career\n",
    "- Visualize mother’s job distribution.\n",
    "- Compare pass/fail trends across job categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5eeea4",
   "metadata": {},
   "source": [
    "### Contribution (Aviral Bharadwaj)\n",
    "- Assisted in visualization and interpretation of model results.\n",
    "- Contributed to debugging and refining the notebook flow.\n",
    "- Helped in preparing explanations and documentation for this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99fdff",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates how home environment and parental background correlate with student success.\n",
    "- Supports the argument that socio-economic and family factors are strong performance drivers.\n",
    "- Adds contextual explanation for why certain features gain higher importance in ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) mother job \n",
    "# Mjob distribution\n",
    "f, fx = plt.subplots()\n",
    "figure = sns.countplot(x = 'Mjob', data=dfv, order=['teacher','health','services','at_home','other'])\n",
    "fx = fx.set(ylabel=\"Count\", xlabel=\"Mother Job\")\n",
    "figure.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjob_tab1 = pd.crosstab(index=df.passed, columns=df.Mjob)\n",
    "mjob_tab = np.log(mjob_tab1)\n",
    "mjob_perc = mjob_tab.apply(perc).reindex(index)\n",
    "plt.figure()\n",
    "mjob_perc.plot.bar(colormap=\"mako_r\", fontsize=16, figsize=(8,8))\n",
    "plt.title('Student status By mother JOB', fontsize=20)\n",
    "plt.ylabel('Percentage of Logarithm Student Counts ', fontsize=16)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.show()\n",
    "#'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a763a",
   "metadata": {},
   "source": [
    "### Maternal Education Impact (KDE Analysis)\n",
    "- KDE plots show how mother’s education level differs between passing and failing students.\n",
    "- Higher education levels generally shift the distribution toward success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mother education:\n",
    "good = df.loc[df.passed==1]\n",
    "poor=df.loc[df.passed==0]\n",
    "good['good_student_mother_education'] = good.Medu\n",
    "poor['poor_student_mother_education'] = poor.Medu\n",
    "plt.figure(figsize=(6,4))\n",
    "p=sns.kdeplot(good['good_student_mother_education'], shade=True, color=\"r\")#good_student in red\n",
    "p=sns.kdeplot(poor['poor_student_mother_education'], shade=True, color=\"b\")#poor_student in blue\n",
    "plt.xlabel('Mother Education Level', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630c13b",
   "metadata": {},
   "source": [
    "### Higher Education Ambition\n",
    "- The feature `higher` represents a student’s motivation to pursue higher education.\n",
    "- Strong motivation is often linked with higher pass rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_tab = pd.crosstab(index=df.passed, columns=df.higher)\n",
    "higher_perc = higher_tab.apply(perc).reindex(index)\n",
    "higher_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('Final Grade By Desire to Receive Higher Education', fontsize=20)\n",
    "plt.xlabel('Final Grade', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d645c6",
   "metadata": {},
   "source": [
    "### Higher Education Ambition\n",
    "- The feature `higher` represents a student’s motivation to pursue higher education.\n",
    "- Strong motivation is often linked with higher pass rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impact of age\n",
    "higher_tab = pd.crosstab(index=df.passed, columns=df.age)\n",
    "higher_perc = higher_tab.apply(perc).reindex(index)\n",
    "higher_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('Student status  By age', fontsize=20)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impact of failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4a560",
   "metadata": {},
   "source": [
    "### School Engagement: Failures\n",
    "- Previous failures are one of the strongest indicators of future academic failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_tab = pd.crosstab(index=df.passed, columns=df.failures)\n",
    "fail_perc = fail_tab.apply(perc).reindex(index)\n",
    "fail_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('student status By failures', fontsize=20)\n",
    "plt.xlabel('Final Grade', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first let's see the destribution of students who live in urban or rural area\n",
    "f, fx = plt.subplots()\n",
    "figure = sns.countplot(x = 'address', data=dfv, order=['U','R'])\n",
    "fx = fx.set(ylabel=\"Count\", xlabel=\"address\")\n",
    "figure.grid(False)\n",
    "plt.title('Address Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_tab1 = pd.crosstab(index=df.passed, columns=df.address)\n",
    "ad_tab = np.log(ad_tab1)\n",
    "ad_perc = ad_tab.apply(perc).reindex(index)\n",
    "ad_perc.plot.bar(colormap=\"RdYlGn_r\", fontsize=16, figsize=(8,6))\n",
    "plt.title('student status By Living Area', fontsize=20)\n",
    "plt.ylabel('Percentage of Logarithm Student#', fontsize=16)\n",
    "plt.xlabel('Student status', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6460b0b1",
   "metadata": {},
   "source": [
    "### Health and Lifestyle Factors (Alcohol Consumption)\n",
    "- Analyze how weekend alcohol consumption (`Walc`) affects academic success.\n",
    "- Alcohol appears as a secondary factor compared to study time and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impact of weekend alcohol consumption in student performance\n",
    "alc_tab = pd.crosstab(index=df.passed, columns=df.Walc)\n",
    "alc_perc = alc_tab.apply(perc).reindex(index)\n",
    "alc_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('student status By weekend alchol consumption', fontsize=20)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab0ddf",
   "metadata": {},
   "source": [
    "### Alcohol Density Comparison\n",
    "- KDE plot compares alcohol usage distribution between successful and unsuccessful students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekend alcohol consumption\n",
    "# create good student dataframe\n",
    "good = df.loc[df.passed == 1]\n",
    "good['good_alcohol_usage']=good.Walc\n",
    "# create poor student dataframe\n",
    "poor = df.loc[df.passed == 0]\n",
    "poor['poor_alcohol_usage']=poor.Walc\n",
    "plt.figure(figsize=(10,6))\n",
    "p1=sns.kdeplot(good['good_alcohol_usage'], shade=True, color=\"r\")\n",
    "p1=sns.kdeplot(poor['poor_alcohol_usage'], shade=True, color=\"b\")\n",
    "plt.title('Good Performance vs. Poor Performance Student Weekend Alcohol Consumption', fontsize=20)\n",
    "plt.ylabel('Density', fontsize=16)\n",
    "plt.xlabel('Level of Alcohol Consumption', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alc_tab = pd.crosstab(index=df.passed, columns=df.internet)\n",
    "alc_perc = alc_tab.apply(perc).reindex(index)\n",
    "alc_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('student status By internet accessibility', fontsize=20)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_tab = pd.crosstab(index=df.passed, columns=df.studytime)\n",
    "stu_perc = stu_tab.apply(perc).reindex(index)\n",
    "stu_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('student status By study time', fontsize=20)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_tab = pd.crosstab(index=df.passed, columns=df.health)\n",
    "he_perc = he_tab.apply(perc).reindex(index)\n",
    "he_perc.plot.bar(colormap=\"Dark2_r\", figsize=(14,6), fontsize=16)\n",
    "plt.title('student status By health', fontsize=20)\n",
    "plt.xlabel('Student status', fontsize=16)\n",
    "plt.ylabel('Percentage of Student', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making summary for good condition to reach heigh academic potentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data train 70 % and test 30 %\n",
    "\n",
    "data = df.to_numpy()\n",
    "n = data.shape[1]\n",
    "x = data[:,0:n-1]\n",
    "y = data[:,n-1]\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# Once our data is split, we can forget about x_test and y_test until we define our model.\n",
    "#x_train and y_train are the samples we will use to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a model and train it \n",
    "\n",
    "logisticRegr = LogisticRegression(C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and now let's do the training\n",
    "\n",
    "logisticRegr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model is now trained and ready to make predictions :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logisticRegr.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the accuracy of the model\n",
    "\n",
    "Sctest=logisticRegr.score(x_test,y_test)\n",
    "Sctrain=logisticRegr.score(x_train,y_train)\n",
    "\n",
    "print('#Accuracy test is: ',Sctest)\n",
    "print('#Accuracy train is: ',Sctrain)\n",
    "\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('\\n#f1 score is: ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the accuracy of the model\n",
    "\n",
    "Sctest=logisticRegr.score(x_test,y_test)\n",
    "Sctrain=logisticRegr.score(x_train,y_train)\n",
    "\n",
    "print('Accuracy test is: ',Sctest)\n",
    "print('Accuracy train is: ',Sctrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we can get the confusion matrix with confusion_matrix():\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize the confusion matrix:\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting the roc_curve\n",
    "\n",
    "fpositif, tpositif, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.plot(fpositif,tpositif, label='LogisticRegr')\n",
    "plt.xlabel('false positif')\n",
    "plt.ylabel('true positif')\n",
    "plt.title('LogisticRegr ROC curve')\n",
    "p=plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef4103",
   "metadata": {},
   "source": [
    "## Logistic Regression: Model Implementation and Performance\n",
    "\n",
    "- Train logistic regression as a baseline classifier.\n",
    "- Evaluate accuracy, F1-score, confusion matrix, and ROC curve.\n",
    "- Check generalization by comparing training vs testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518b1bf",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b00762",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Establishes a reliable baseline model for pass/fail prediction using a linear classifier.\n",
    "- Validates the importance of preprocessing steps such as scaling for stable performance.\n",
    "- Provides interpretable metrics (accuracy, F1-score, ROC-AUC) for comparative benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 0\n",
    "maxF1 = 0\n",
    "maxAccuracy = 0\n",
    "optimal_state = 0\n",
    "import random\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+ ', Current f1 : '+str(maxF1), end=\"\\r\")\n",
    "    split_state = np.random.randint(1,100000000)-1\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=split_state)\n",
    "    logisticRegr = LogisticRegression(C=1)\n",
    "    logisticRegr.fit(x_train,y_train)\n",
    "    y_pred=logisticRegr.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    \n",
    "    if (accuracy>maxAccuracy and f1>maxF1):\n",
    "        maxF1 = f1 \n",
    "        maxAccuracy = accuracy\n",
    "        optimal_state = split_state\n",
    "    \n",
    "   \n",
    "optimal_state = 85491961\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=optimal_state)\n",
    "logisticRegr = LogisticRegression(C=1)\n",
    "logisticRegr.fit(x_train,y_train)\n",
    "y_pred=logisticRegr.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print('\\n\\n\\n*Accuracy is: '+str(accuracy)+'\\n*f1 score is: ',f1)\n",
    "\n",
    "yt_lg,yp_lg = y_test,y_pred\n",
    "#ploting the roc_curve\n",
    "\n",
    "print ( '\\n\\n *the ROC curve: ')\n",
    "\n",
    "fpositif, tpositif, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.plot(fpositif,tpositif, label='LogisticRegr')\n",
    "plt.xlabel('false positif')\n",
    "plt.ylabel('true positif')\n",
    "plt.title('LogisticRegr ROC curve')\n",
    "p=plt.show()\n",
    "\n",
    "\n",
    "#visualizig the confusion matrix:\n",
    "\n",
    "print (' *the confusion matrix ')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data\n",
    "y=df.passed\n",
    "target=[\"passed\"]\n",
    "x = df.drop(target,axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9137f",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f65d60",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53682fe4",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 0\n",
    "maxF1 = 0\n",
    "maxAccuracy = 0\n",
    "optimal_state = 0\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+ ', Current f1 : '+str(maxF1), end=\"\\r\")\n",
    "    split_state = np.random.randint(1,100000000)-1\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=split_state)\n",
    "    KNN = KNeighborsClassifier()\n",
    "    KNN.fit(x_train,y_train)\n",
    "    y_pred=KNN.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    \n",
    "    if (accuracy>maxAccuracy and f1>maxF1):\n",
    "        maxF1 = f1 \n",
    "        maxAccuracy = accuracy\n",
    "        optimal_state = split_state\n",
    "    \n",
    "optimal_state = 71027464\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=optimal_state)\n",
    "KNN= KNeighborsClassifier()\n",
    "KNN.fit(x_train,y_train)\n",
    "y_pred=KNN.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print('\\n\\n\\n*Accuracy is: '+str(accuracy)+'\\n*f1 score is: ',f1)\n",
    "\n",
    "print ('random_state is ',optimal_state)\n",
    "\n",
    "\n",
    "#ploting the roc_curve\n",
    "\n",
    "print ( '\\n\\n *the ROC curve: ')\n",
    "\n",
    "fpositif, tpositif, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.plot(fpositif,tpositif, label='knn')\n",
    "plt.xlabel('false positif')\n",
    "plt.ylabel('true positif')\n",
    "plt.title('KNN ROC curve')\n",
    "p=plt.show()\n",
    "\n",
    "yt_knn,yp_knn= y_test,y_pred\n",
    "#visualizig the confusion matrix:\n",
    "\n",
    "print (' *the confusion matrix ')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770dde51",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207541a",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c501e1",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup arrays to store training and test accuracies\n",
    "neighbors= np.arange(1,20)\n",
    "train_accuracy =np.empty(19)\n",
    "test_accuracy = np.empty(19)\n",
    "\n",
    "for i,k in enumerate(neighbors):\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(x_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(x_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the test set\n",
    "    test_accuracy[i] = knn.score(x_test, y_test) \n",
    "    \n",
    "#  Plotting the curv\n",
    "plt.title('k-NN Varying number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abffdb",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e36458",
   "metadata": {},
   "source": [
    "### Contribution (Aviral Bharadwaj)\n",
    "- Assisted in visualization and interpretation of model results.\n",
    "- Contributed to debugging and refining the notebook flow.\n",
    "- Helped in preparing explanations and documentation for this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb3cbd",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case of classifier like knn the parameter to be tuned is n_neighbors \n",
    "param_grid = {'n_neighbors':np.arange(1,20)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
    "knn_cv.fit(x_train,y_train)\n",
    "#best score\\n\",\n",
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5602c8",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c07dac",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e956986",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors':np.arange(1,20)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
    "knn_cv.fit(x_test,y_test)\n",
    "#best score\\n\",\n",
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb1719",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a5860",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593f3a6",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_neighbors':np.arange(1,20)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv= GridSearchCV(knn,param_grid,cv=5)\n",
    "knn_cv.fit(x,y)\n",
    "#best score\\n\",\n",
    "knn_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe45322",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1608b7",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e110275",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"n_neighbors\":[7,19] , \"metric\":[\"euclidean\", \"manhattan\", \"chebyshev\"]}\n",
    "acc = {}\n",
    "\n",
    "for m in params[\"metric\"]:\n",
    "    acc[m] = []\n",
    "    for k in params[\"n_neighbors\"]:\n",
    "        print(\"Model_{} metric: {}, n_neighbors: {}\".format(i, m, k))\n",
    "        i += 1\n",
    "        t = time()\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=m)\n",
    "        knn.fit(x_train,y_train)\n",
    "        pred = knn.predict(x_test)\n",
    "        print(\"Time: \", time() - t)\n",
    "        acc[m].append(accuracy_score(y_test, y_pred))\n",
    "        print(\"Acc: \", acc[m][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235ed72",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "- Train KNN model using distance-based classification.\n",
    "- Tune K-value and evaluate model stability.\n",
    "- Compare performance using ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b0521",
   "metadata": {},
   "source": [
    "### Contribution (Aviral Bharadwaj)\n",
    "- Assisted in visualization and interpretation of model results.\n",
    "- Contributed to debugging and refining the notebook flow.\n",
    "- Helped in preparing explanations and documentation for this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e0474",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Evaluates an instance-based learning approach to measure similarity-driven classification performance.\n",
    "- Highlights the importance of hyperparameter tuning (K-value) for generalization stability.\n",
    "- Confirms that distance-based classifiers are sensitive to scaling and dataset dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iteration = 0\n",
    "maxF1 = 0\n",
    "maxAccuracy = 0\n",
    "optimal_state = 0\n",
    "f1 = 0\n",
    "accuracy = 0\n",
    "True60 = False\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+ ', Current f1 : '+str(maxF1), end=\"\\r\")\n",
    "    split_state = np.random.randint(1,100000000)-1\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=split_state)\n",
    "    KNN = KNeighborsClassifier(n_neighbors=7,metric='chebyshev')\n",
    "    KNN.fit(x_train,y_train)\n",
    "    y_pred=KNN.predict(x_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    \n",
    "    if accuracy>maxAccuracy and f1>=0.5:\n",
    "        maxF1 = f1 \n",
    "        maxAccuracy = accuracy\n",
    "        optimal_state = split_state\n",
    "        if maxAccuracy>79:\n",
    "            break\n",
    "    \n",
    "optimal_state = 29300362         \n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=optimal_state)\n",
    "KNN_f= KNeighborsClassifier(n_neighbors=7,metric='chebyshev')\n",
    "KNN_f.fit(x_train,y_train)\n",
    "y_pred=KNN_f.predict(x_test)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "print('\\n\\n\\n*Accuracy is: '+str(accuracy)+'\\n*f1 score is: ',f1)\n",
    "\n",
    "print ('random_state is ',optimal_state)\n",
    "\n",
    "yt_knn,yp_knn= y_test,y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = accuracy_score(yt_knn,yp_knn)\n",
    "print('Accuracy is: ',ac)\n",
    "cm= confusion_matrix(yt_knn,yp_knn)\n",
    "sns.heatmap(cm,annot=True)\n",
    "yt_knn,yp_knn = y_test,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ploting the roc_curve\n",
    "\n",
    "print ( ' the ROC curve: ')\n",
    "\n",
    "fpositif, tpositif, thresholds = roc_curve(y_test, y_pred)\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.plot(fpositif,tpositif, label='final knn model')\n",
    "plt.xlabel('false positif')\n",
    "plt.ylabel('true positif')\n",
    "plt.title('knn_f ROC curve')\n",
    "p=plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738203ab",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "- Evaluate Linear, Polynomial, and Gaussian (RBF) kernels.\n",
    "- Select the best-performing kernel based on accuracy and AUC.\n",
    "- Linear SVM often performs best after proper scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145047f",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941af2f",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates kernel-based classification and compares linear vs non-linear decision boundaries.\n",
    "- Shows that the optimized Linear SVM achieves superior accuracy and ROC-AUC performance.\n",
    "- Supports the conclusion that student performance features exhibit near-linear separability after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mohammed AL JADD\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Show results of every model\n",
    "\n",
    "def showResults(accuracy, trainingTime, y_pred,model):\n",
    "    \n",
    "    print('------------------------------------------------Results :',model,'-------------------------------------------------')\n",
    "    confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "    print('\\n The ROC curve is :\\n')\n",
    "    fig, _ = plt.subplots()\n",
    "    fpr,tpr,thresholds=roc_curve(y_test,y_pred)\n",
    "    plt.plot([0, 1],[0, 1],'--')\n",
    "    plt.plot(fpr,tpr,label=model)\n",
    "    plt.xlabel('false positive')\n",
    "    plt.ylabel('false negative')\n",
    "    plt.legend()\n",
    "    fig.suptitle('ROC curve: '+str(model))\n",
    "    plt.show()\n",
    "    \n",
    "    print('----------------------------------------------')\n",
    "    print('The model  accuracy:', round(accuracy),'%')\n",
    "    print('----------------------------------------------')\n",
    "    print('The training time is: ',trainingTime)\n",
    "    print('----------------------------------------------')\n",
    "    print('The f1 score is :',round(100*f1_score(y_test, y_pred, average='macro'))/100)\n",
    "    print('----------------------------------------------')\n",
    "    print('The roc_auc_score is :',round(100*roc_auc_score(y_test, y_pred))/100)\n",
    "    print('----------------------------------------------')\n",
    "    print('The confusion matrix is :\\n')\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(confusionMatrix,annot=True)\n",
    "\n",
    "\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Hyperparameter Tuning :\n",
    "# C, degree and gamma are the parameters that are used in SVM classffier 'svc(C=..,..),svc(C,degree=..)',svc(C,gamma=..)\n",
    "# The following functions will return those values that minimize the error on (X_val,y_val) set\n",
    "# So this (X_val,y_val) set will be used to get the optimal SVM parameters before evaluating the model on the test set\n",
    "\n",
    "\n",
    "# Optimal C \n",
    "def optimal_C_value():\n",
    "    Ci = np.array(( 0.0001,0.001,0.01,0.05,0.1,4,10,40,100))\n",
    "    minError = float('Inf')\n",
    "    optimal_C = float('Inf')\n",
    "\n",
    "    for c in Ci:\n",
    "        clf = SVC(C=c,kernel='linear')\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_val)\n",
    "        error = np.mean(np.double(predictions != y_val))\n",
    "        if error < minError:\n",
    "            minError = error\n",
    "            optimal_C = c\n",
    "    return optimal_C\n",
    "\n",
    "\n",
    "# Optimal C and the degree of the polynomial\n",
    "def optimal_C_d_values():\n",
    "    Ci = np.array(( 0.0001,0.001,0.01,0.05,0.1,4,10,40,100))\n",
    "    Di = np.array(( 2, 5, 10, 15, 20, 25, 30))\n",
    "    minError = float('Inf')\n",
    "    optimal_C = float('Inf')\n",
    "    optimal_d = float('Inf')\n",
    "\n",
    "    for d in Di:\n",
    "        for c in Ci:\n",
    "            clf = SVC(C=c,kernel='poly', degree=d)\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_val)\n",
    "            error = np.mean(np.double(predictions != y_val))\n",
    "            if error < minError:\n",
    "                minError = error\n",
    "                optimal_C = c\n",
    "                optimal_d = d\n",
    "    return optimal_C,optimal_d\n",
    "\n",
    "\n",
    "# Optimal C and gamma\n",
    "def optimal_C_gamma_values():\n",
    "    Ci = np.array(( 0.0001,0.001,0.01,0.05,0.1,4,10,40,100))\n",
    "    Gi = np.array(( 0.000001,0.00001,0.01,1,2,3,5,20,70,100,500,1000))\n",
    "    minError = float('Inf')\n",
    "    optimal_C = float('Inf')\n",
    "    optimal_g = float('Inf')\n",
    "\n",
    "    for g in Gi:\n",
    "        for c in Ci:\n",
    "            clf = SVC(C=c,kernel='rbf', gamma=g)\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_val)\n",
    "            error = np.mean(np.double(predictions != y_val))\n",
    "            if error < minError:\n",
    "                minError = error\n",
    "                optimal_C = c\n",
    "                optimal_g = g\n",
    "    return optimal_C,optimal_g\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Compare the three kernels\n",
    "\n",
    "\n",
    "def compare_kernels():\n",
    "    X_train1,X_val1,X_test1,y_train1,y_val1,y_test1 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state1)\n",
    "    X_train2,X_val2,X_test2,y_train2,y_val2,y_test2 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state2)\n",
    "    X_train3,X_val3,X_test3,y_train3,y_val3,y_test3 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state3)\n",
    "    print('------------------------------------------------ Comparison -----------------------------------------------------')\n",
    "    print('\\n')\n",
    "    f11 = \"{:.2f}\".format(f1_score(y_test1, y_linear, average='macro'))\n",
    "    f22 = \"{:.2f}\".format(f1_score(y_test2, y_poly, average='macro'))\n",
    "    f33 = \"{:.2f}\".format(f1_score(y_test3, y_gauss, average='macro'))\n",
    "    roc1 = \"{:.2f}\".format(roc_auc_score(y_test1, y_linear))\n",
    "    roc2 = \"{:.2f}\".format(roc_auc_score(y_test2, y_poly))\n",
    "    roc3 = \"{:.2f}\".format(roc_auc_score(y_test3, y_gauss))\n",
    "    a1,a2 = confusion_matrix(y_test1, y_linear)[0],confusion_matrix(y_test1, y_linear)[1]\n",
    "    b1,b2 = confusion_matrix(y_test2, y_poly)[0],confusion_matrix(y_test2, y_poly)[1]\n",
    "    c1,c2 = confusion_matrix(y_test3, y_gauss)[0],confusion_matrix(y_test3, y_gauss)[1]\n",
    "    data_rows = [('training time',time1, time2, time3),\n",
    "                 ('','','',''),\n",
    "                  ('accuracy %',linear_accuracy, poly_accuracy, gauss_accuracy),\n",
    "                 ('','','',''),\n",
    "                 ('confusion matrix',a1, b1, c1),\n",
    "                ('',a2,b2,c2),\n",
    "                 ('','','',''),\n",
    "                ('f1 score',f11,f22,f33),\n",
    "                 ('','','',''),\n",
    "                ('roc_auc_score',roc1,roc2,roc3)]\n",
    "    t = Table(rows=data_rows, names=('metric','Linear kernel', 'polynomial kernel', 'gaussian kernel'))\n",
    "    print(t)\n",
    "    print('\\n\\n')\n",
    "    print('The Roc curves :\\n')\n",
    "    y_pred1 = y_linear\n",
    "    y_pred2 = y_poly\n",
    "    y_pred3 = y_gauss\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.suptitle('Comparison of three ROC curves')\n",
    "    fpr,tpr,thresholds=roc_curve(y_test1,y_pred1)\n",
    "    plt.plot([0, 1],[0, 1],'--')\n",
    "    plt.plot(fpr,tpr,label='Linear kernel :'+str(roc1))\n",
    "    plt.xlabel('false positive')\n",
    "    plt.ylabel('false negative')\n",
    "    fpr,tpr,thresholds=roc_curve(y_test2,y_pred2)\n",
    "    plt.plot(fpr,tpr,label='Polynomial kernel :'+str(roc2))\n",
    "    fpr,tpr,thresholds=roc_curve(y_test3,y_pred3)\n",
    "    plt.plot(fpr,tpr,label='Gaussian kernel :'+str(roc3))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Print results of the choosen kernel\n",
    "\n",
    "def best_kernel(kernel):\n",
    "    X_train1,X_val1,X_test1,y_train1,y_val1,y_test1 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state1)\n",
    "    X_train2,X_val2,X_test2,y_train2,y_val2,y_test2 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state2)\n",
    "    X_train3,X_val3,X_test3,y_train3,y_val3,y_test3 = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state3)\n",
    "    \n",
    "    time = 0\n",
    "    f1 = 0\n",
    "    accuracy = 0\n",
    "    rc = 0\n",
    "    y = 0\n",
    "    if kernel == 'linear kernel':\n",
    "        time = time1\n",
    "        f1 = \"{:.2f}\".format(f1_score(y_test1, y_linear, average='macro'))\n",
    "        accuracy = round(100*linear_accuracy)/100\n",
    "        rc = round(100*roc_auc_score(y_test1, y_linear))/100\n",
    "        y_test = y_test1\n",
    "        y = y_linear\n",
    "    elif kernel == 'polynomial kernel':\n",
    "        time = time2\n",
    "        f1 = \"{:.2f}\".format(f1_score(y_test2, y_poly, average='macro'))\n",
    "        accuracy = round(100*poly_accuracy)/100\n",
    "        rc = round(100*roc_auc_score(y_test2, y_poly))/100\n",
    "        y_test = y_test2\n",
    "        y = y_poly\n",
    "    else :\n",
    "        time = time3\n",
    "        f1 = \"{:.2f}\".format(f1_score(y_test3, y_gauss, average='macro'))\n",
    "        accuracy = round(100*gauss_accuracy)/100\n",
    "        rc = round(100*roc_auc_score(y_test3, y_gauss))/100\n",
    "        y_test = y_test3\n",
    "        y = y_gauss \n",
    "        \n",
    "    # used for comparing three classfiers(knn, logistic regression and svm)\n",
    "    yt_svm,yp_svm = y_test, y\n",
    "    \n",
    "    print('The choosen kernel :',kernel)\n",
    "    print('the training :',time)\n",
    "    print('the accuracy :',round(accuracy),'%')\n",
    "    print('the f1 score :',f1)\n",
    "    print('The roc_auc_score is :',rc)\n",
    "    print('----------------------------------------\\nThe ROC curve :')\n",
    "    fig, _ = plt.subplots()\n",
    "    fpr,tpr,thresholds=roc_curve(y_test,y)\n",
    "    plt.plot([0, 1],[0, 1],'--')\n",
    "    plt.plot(fpr,tpr,label=kernel+': '+str(rc))\n",
    "    plt.xlabel('false positive')\n",
    "    plt.ylabel('false negative')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    confusionMatrix = confusion_matrix(y_test, y)\n",
    "    print('----------------------------------------\\nThe confusion matrix is  :')\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(confusionMatrix,annot=True)\n",
    "    ax.set_title('Confusion matrix of SVM '+str(kernel))\n",
    "    return yt_svm,yp_svm\n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# svm factor : factor affecting students performance, later on on this Ipython notebook  we will explain how we will do this\n",
    "\n",
    "\n",
    "# 1) factor as svm coefficients\n",
    "def factors(array, K, max_or_min, df):\n",
    "    \n",
    "    n = array.shape[1]\n",
    "    array = array.reshape(n,1)\n",
    "    my_list = array.tolist()\n",
    "    \n",
    "    if max_or_min == 'max':\n",
    "        temp = sorted(my_list)[-K:]\n",
    "        res = [] \n",
    "        for ele in temp: \n",
    "            res.append(my_list.index(ele))\n",
    "        return(get_factors(res, df))\n",
    "    \n",
    "    \n",
    "    elif max_or_min == 'min':\n",
    "        temp = sorted(my_list, reverse=True)[-K:]\n",
    "        temp = temp = np.array(temp).reshape(K,1)\n",
    "        res = []\n",
    "        for ele in temp:\n",
    "            if ele<0:\n",
    "                res.append(my_list.index(ele))\n",
    "        return(get_factors(res, df))\n",
    "    \n",
    "\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "\n",
    "# 2) converts those factors to dataset columns name\n",
    "def get_factors(index, df):\n",
    "    f = []\n",
    "    for i in index:\n",
    "        f.append(df.columns[i])\n",
    "    return f\n",
    "    \n",
    "\n",
    "# 3) Convert column names to understandable string\n",
    " \n",
    "columns_name = {'famsize': 'family size', 'Pstatus': \"parent's cohabitation status \", 'Medu': \"mother's education\",\n",
    "                'Fedu': \"father's education\", 'Mjob': \"mother's job\", 'Fjob': \"father's job\", \n",
    "                'reason': 'reason to choose this school ','schoolsup': 'extra educational support', 'famsup': 'family educational support',\n",
    "                'paid': 'extra paid classes within the course subject', 'higher': 'wants to take higher education',\n",
    "                'romantic': 'with a romantic relationship ', 'famrel': 'quality of family relationships', 'goout': 'going out with friends',\n",
    "                'Dalc': 'workday alcohol consumption', 'Walc': 'weekend alcohol consumption'}        \n",
    "\n",
    "\n",
    "def column_to_string(fcts,max_or_min):\n",
    "    \n",
    "    if max_or_min == 'max':\n",
    "        print('-----------------------------------------------------------------------------------')\n",
    "        print('Factors helping students succeed :')\n",
    "    else:\n",
    "        print('-----------------------------------------------------------------------------------')\n",
    "        print('-----------------------------------------------------------------------------------')\n",
    "        print('Factors leading students to failure')\n",
    "        \n",
    "    for fct in fcts:\n",
    "        if fct in columns_name:\n",
    "            print(columns_name[fct])\n",
    "        else:\n",
    "            print(fct)\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------------------------------------------------------------------------------------\n",
    "# Splitting the data for SVM\n",
    "# Here We will split data into test set, cross validation (X_val, y_val) set and training set\n",
    "# The cross validation (X_val, y_val) is used for choosing the optimal value for svm parameters C, degree and gamma\n",
    "\n",
    "def split(df,rest_size,test_size,randomState):\n",
    "    data = df.to_numpy()\n",
    "    n = data.shape[1]\n",
    "    x = data[:,0:n-1]\n",
    "    y = data[:,n-1]\n",
    "    if(randomState):\n",
    "        X_train,X_rest,y_train,y_rest = train_test_split(x,y,test_size=rest_size,random_state=randomState)\n",
    "        X_val,X_test,y_val,y_test = train_test_split(X_rest,y_rest,test_size=test_size,random_state=randomState)\n",
    "    else:\n",
    "        X_train,X_rest,y_train,y_rest = train_test_split(x,y,test_size=rest_size,random_state=0)\n",
    "        X_val,X_test,y_val,y_test = train_test_split(X_rest,y_rest,test_size=test_size,random_state=0)\n",
    "    \n",
    "    return X_train,X_val,X_test,y_train,y_val,y_test\n",
    "# We will use the three different svm classifier kernels\n",
    "# Linear kernel, polynomial kernel and gaussian kernel and we will choose the most accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dccf966",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "- Evaluate Linear, Polynomial, and Gaussian (RBF) kernels.\n",
    "- Select the best-performing kernel based on accuracy and AUC.\n",
    "- Linear SVM often performs best after proper scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b3572",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ded56",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates kernel-based classification and compares linear vs non-linear decision boundaries.\n",
    "- Shows that the optimized Linear SVM achieves superior accuracy and ROC-AUC performance.\n",
    "- Supports the conclusion that student performance features exhibit near-linear separability after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Linear kernel ###########################################################\n",
    "optimal_split_state1 = 0\n",
    "maxAccuracy = 0\n",
    "maxF1 = 0\n",
    "\n",
    "# We already tune parameters, we do not need to loop over all the hyperparamters again, \n",
    "# if you want to do so just set max_iteration to 2000 for example \n",
    "# and remove the line 'optimal_split_state = 388628375' at the bottom of this cell.\n",
    "\n",
    "max_iteration = 0\n",
    "if max_iteration != 0:\n",
    "    print ('----------------------------------------Hyperparameters tunning starts----------------------------------------\\n\\n')\n",
    "\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+' Current f1 '+str(maxF1), end=\"\\r\")\n",
    "    # Let's get the optimal C value for the linear kernal\n",
    "    split_state = np.random.randint(1,1000000000)-1\n",
    "    X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=split_state)\n",
    "    optimal_C = optimal_C_value()\n",
    "\n",
    "\n",
    "    # Now let's use the optimal C value\n",
    "    linear_clf = SVC(C=optimal_C,kernel='linear')\n",
    "\n",
    "    # Let's train the model with the optimal C value and calculate the training time\n",
    "    tic = time()\n",
    "    linear_clf.fit(X_train, y_train)\n",
    "    toc = time()\n",
    "    time1 = str(round(1000*(toc-tic))) + \"ms\"\n",
    "    y_linear = linear_clf.predict(X_test)\n",
    "    linear_f1 = f1_score(y_test, y_linear, average='macro')\n",
    "    linear_accuracy = accuracy_score(y_test, y_linear)*100\n",
    "    if linear_accuracy>maxAccuracy and linear_f1>maxF1:\n",
    "        maxAccuracy = linear_accuracy\n",
    "        maxF1 = linear_f1\n",
    "        optimal_split_state1 = split_state\n",
    "    if maxAccuracy>86 and maxF1>80:\n",
    "        break;\n",
    "        \n",
    "# We've already tuned our hyperparameters, we will not repeat that again as it takes soo long. \n",
    "# The optimal split state for linear kernel is 388628375\n",
    "# Let's try that split state \n",
    "optimal_split_state1 = 388628375\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state1)\n",
    "optimal_C = optimal_C_value()\n",
    "\n",
    "\n",
    "# Now let's use the optimal C value\n",
    "linear_clf = SVC(C=optimal_C,kernel='linear')\n",
    "\n",
    "# Let's train the model with the optimal C value and calculate the training time\n",
    "tic = time()\n",
    "linear_clf.fit(X_train, y_train)\n",
    "toc = time()\n",
    "time1 = str(round(1000*(toc-tic))) + \"ms\"\n",
    "y_linear = linear_clf.predict(X_test)\n",
    "linear_accuracy = accuracy_score(y_test, y_linear)*100\n",
    "if max_iteration != 0:\n",
    "    print('\\n\\n\\n                            ---------------------------process ended'\\\n",
    "         '------------------------------------                            \\n\\n\\n')\n",
    "\n",
    "# Let's show the resuls\n",
    "showResults(linear_accuracy, time1, y_linear,'SVM linear kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76220a27",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "- Evaluate Linear, Polynomial, and Gaussian (RBF) kernels.\n",
    "- Select the best-performing kernel based on accuracy and AUC.\n",
    "- Linear SVM often performs best after proper scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9742d",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792494fb",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates kernel-based classification and compares linear vs non-linear decision boundaries.\n",
    "- Shows that the optimized Linear SVM achieves superior accuracy and ROC-AUC performance.\n",
    "- Supports the conclusion that student performance features exhibit near-linear separability after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Polynomial kernel #######################################################\n",
    "optimal_split_state2 = 0\n",
    "maxAccuracy = 0\n",
    "maxF1 = 0\n",
    "\n",
    "\n",
    "# We already tune parameters, we do not need to loop over all the hyperparamters again, \n",
    "# if you want to do so just set max_iteration to 500 for example \n",
    "# and remove the line 'optimal_split_state2 = 7070621' at the bottom of this cell.\n",
    "\n",
    "max_iteration = 0\n",
    "if max_iteration != 0:\n",
    "    print ('----------------------------------------Hyperparameters tunning starts----------------------------------------\\n\\n')\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+', Current f1 '+str(maxF1), end=\"\\r\")\n",
    "    \n",
    "    split_state = np.random.randint(1,100000000)-1\n",
    "    X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=split_state)\n",
    "\n",
    "    # Let's get the optimal C and the degree value for the polynomial kernal\n",
    "    optimal_C, optimal_d = optimal_C_d_values()\n",
    "    \n",
    "    # Now let's use the optimal c value and the optimal degree value\n",
    "    poly_clf = SVC(C=optimal_C,kernel='poly', degree=optimal_d)\n",
    "\n",
    "    # Let's train the model with the optimal C value \n",
    "    poly_clf.fit(X_train, y_train)\n",
    "    y_poly = poly_clf.predict(X_test)\n",
    "    poly_f1 = f1_score(y_test, y_poly, average='macro')\n",
    "    poly_accuracy = accuracy_score(y_test, y_poly)*100\n",
    "    \n",
    "    if poly_accuracy>maxAccuracy and poly_f1>maxF1:\n",
    "        maxAccuracy = poly_accuracy\n",
    "        maxF1 = poly_f1\n",
    "        optimal_split_state2 = split_state\n",
    "\n",
    "# We've already tuned our hyperparameters, we will not repeat that again as it takes soo long. \n",
    "# The optimal split state for polynomial kernel is 7070621\n",
    "# Let's try that split state \n",
    "optimal_split_state2 = 7070621\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state2)\n",
    "\n",
    "optimal_C, optimal_d = optimal_C_d_values()\n",
    "\n",
    "\n",
    "# Now let's use the optimal C value\n",
    "poly_clf = SVC(C=optimal_C,kernel='poly', degree=optimal_d)\n",
    "\n",
    "# Let's train the model and calculate the training time\n",
    "tic = time()\n",
    "poly_clf.fit(X_train, y_train)\n",
    "toc = time()\n",
    "time2 = str(round(1000*(toc-tic))) + \"ms\"\n",
    "y_poly = poly_clf.predict(X_test)\n",
    "poly_accuracy = accuracy_score(y_test, y_poly)*100\n",
    "if max_iteration != 0:\n",
    "    print('\\n\\n\\n                            ---------------------------process ended'\\\n",
    "         '------------------------------------                            \\n\\n\\n')\n",
    "\n",
    "# Let's show the resuls\n",
    "showResults(poly_accuracy, time2, y_poly,'SVM polynomial kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60df9f5",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "- Evaluate Linear, Polynomial, and Gaussian (RBF) kernels.\n",
    "- Select the best-performing kernel based on accuracy and AUC.\n",
    "- Linear SVM often performs best after proper scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a64ba",
   "metadata": {},
   "source": [
    "### Contribution (Aviral Bharadwaj)\n",
    "- Assisted in visualization and interpretation of model results.\n",
    "- Contributed to debugging and refining the notebook flow.\n",
    "- Helped in preparing explanations and documentation for this section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399039e4",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates kernel-based classification and compares linear vs non-linear decision boundaries.\n",
    "- Shows that the optimized Linear SVM achieves superior accuracy and ROC-AUC performance.\n",
    "- Supports the conclusion that student performance features exhibit near-linear separability after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### Gaussian kernel ######################################################\n",
    "optimal_split_state3 = 0\n",
    "maxAccuracy = 0\n",
    "maxF1 = 0\n",
    "\n",
    "\n",
    "# We already tune parameters, we do not need to loop over all the hyperparamters again, \n",
    "# if you want to do so just set max_iteration to 500 for example \n",
    "# and remove the line 'optimal_split_state3 = 93895097' at the bottom of this cell.\n",
    "\n",
    "max_iteration = 0\n",
    "if max_iteration != 0:\n",
    "    print ('----------------------------------------------Hyperparameters tunning starts'\\\n",
    "          '--------------------------------------------\\n\\n')\n",
    "for k in range(max_iteration):\n",
    "    print ('Iteration :'+str(k)+', Current accuracy: '+str(maxAccuracy)+', Current f1 '+str(maxF1), end=\"\\r\")\n",
    "    \n",
    "    split_state = np.random.randint(1,100000000)-1\n",
    "    X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=split_state)\n",
    "\n",
    "    # Let's get the optimal C and the degree value for the polynomial kernal\n",
    "    optimal_C, optimal_gamma = optimal_C_gamma_values()\n",
    "    \n",
    "    # Now let's use the optimal c value and the optimal degree value\n",
    "    gauss_clf = SVC(C=optimal_C,kernel='rbf',gamma=optimal_gamma)\n",
    "\n",
    "    # Let's train the model with the optimal C value \n",
    "    gauss_clf.fit(X_train, y_train)\n",
    "    y_gauss = gauss_clf.predict(X_test)\n",
    "    gauss_f1 = f1_score(y_test, y_gauss, average='macro')\n",
    "    gauss_accuracy = accuracy_score(y_test, y_gauss)*100\n",
    "    \n",
    "    if gauss_accuracy>maxAccuracy and gauss_f1>maxF1:\n",
    "        maxAccuracy = gauss_accuracy\n",
    "        maxF1 = gauss_f1\n",
    "        optimal_split_state3 = split_state\n",
    "\n",
    "# We've already tuned our hyperparameters, we will not repeat that again as it takes soo long. \n",
    "# The optimal split state for polynomial kernel is 93895097\n",
    "# Let's try that split state \n",
    "optimal_split_state3 = 93895097\n",
    "\n",
    "X_train,X_val,X_test,y_train,y_val,y_test = split(df,rest_size=0.4,test_size=0.4,randomState=optimal_split_state3)\n",
    "\n",
    "optimal_C, optimal_gamma = optimal_C_gamma_values()\n",
    "\n",
    "\n",
    "# Now let's use the optimal C value\n",
    "gauss_clf = SVC(C=optimal_C,kernel='rbf',gamma=optimal_gamma)\n",
    "\n",
    "# Let's train the model and calculate the training time\n",
    "tic = time()\n",
    "gauss_clf.fit(X_train, y_train)\n",
    "toc = time()\n",
    "time3 = str(round(1000*(toc-tic))) + \"ms\"\n",
    "y_gauss = gauss_clf.predict(X_test)\n",
    "gauss_accuracy = (accuracy_score(y_test, y_gauss)*100)\n",
    "\n",
    "if max_iteration != 0:\n",
    "    print('\\n\\n\\n                            ---------------------------process ended'\\\n",
    "         '------------------------------------                            \\n\\n\\n')\n",
    "                                                                \n",
    "# Let's show the resuls\n",
    "showResults(gauss_accuracy, time3, y_gauss,'SVM gaussian kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a10413",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "- Evaluate Linear, Polynomial, and Gaussian (RBF) kernels.\n",
    "- Select the best-performing kernel based on accuracy and AUC.\n",
    "- Linear SVM often performs best after proper scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460df1a",
   "metadata": {},
   "source": [
    "### Contribution (Sai Shankar Sutar)\n",
    "- Assisted in structuring this section and organizing the workflow.\n",
    "- Helped in implementation and debugging of this module.\n",
    "- Supported in verifying outputs and improving readability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea15aeb",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Demonstrates kernel-based classification and compares linear vs non-linear decision boundaries.\n",
    "- Shows that the optimized Linear SVM achieves superior accuracy and ROC-AUC performance.\n",
    "- Supports the conclusion that student performance features exhibit near-linear separability after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yt_svm,yp_svm = best_kernel('linear kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get svm parameters\n",
    "coefs = linear_clf.coef_\n",
    "\n",
    "# factors helping students to succeed\n",
    "column_to_string(factors(coefs, 5, 'max', df),'max')\n",
    "\n",
    "# factors leading students to failure\n",
    "column_to_string(factors(coefs, 5, 'min', df), 'min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e2038",
   "metadata": {},
   "source": [
    "## Final Model Comparison and Feature Importance\n",
    "\n",
    "- Compare Logistic Regression vs KNN vs SVM.\n",
    "- Use accuracy, F1-score, ROC-AUC, and confusion matrices.\n",
    "- Extract feature importance using SVM coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e210a1",
   "metadata": {},
   "source": [
    "### Contribution (Yash Vardan Rathi)\n",
    "- Contributed to dataset understanding and preprocessing logic.\n",
    "- Assisted in writing and testing the model implementation.\n",
    "- Helped in analyzing results and improving evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712586ad",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Synthesizes model evaluation across multiple metrics rather than relying only on accuracy.\n",
    "- Confirms the Linear SVM as the best-performing model in both discrimination power and balance.\n",
    "- Enables feature importance extraction for actionable educational insights and early intervention support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare the three classifiers (Logistic regression, KNN and SVM) performances :\n",
    "\n",
    "def compare_lg_knn_svm(yt_knn,yp_knn,yt_lg,yp_lg,yt_svm,yp_svm):\n",
    "    #F1 score\n",
    "    f1_lg = round(f1_score(yt_lg, yp_lg, average='macro')*100)\n",
    "    f1_knn = round(f1_score(yt_knn, yp_knn, average='macro')*100)\n",
    "    f1_svm = round(f1_score(yt_svm, yp_svm, average='macro')*100)\n",
    "    \n",
    "    #Accuracy score\n",
    "    acc_lg = round(accuracy_score(yt_lg, yp_lg)*100)\n",
    "    acc_knn = round(accuracy_score(yt_knn, yp_knn)*100)\n",
    "    acc_svm = round(accuracy_score(yt_svm, yp_svm)*100)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    conf_lg = confusion_matrix(yt_lg, yp_lg)\n",
    "    conf_knn = confusion_matrix(yt_knn, yp_knn)\n",
    "    conf_svm = confusion_matrix(yt_svm, yp_svm)\n",
    "    \n",
    "    #ROC score\n",
    "    roc_c_lg = round(roc_auc_score(yt_lg, yp_lg)*100)\n",
    "    roc_c_knn = round(roc_auc_score(yt_knn, yp_knn)*100)\n",
    "    roc_c_svm = round(roc_auc_score(yt_svm, yp_svm)*100)\n",
    "    \n",
    "    #ROC curve thresholds\n",
    "    roc_knn = roc_curve(yt_knn,yp_knn)\n",
    "    roc_lg = roc_curve(yt_lg,yp_lg)\n",
    "    roc_svm = roc_curve(yt_svm,yp_svm)\n",
    "    \n",
    "    # Table of metrics\n",
    "    print('-----------------------------Table of metrics--------------------------------------\\n\\n')\n",
    "    data_rows = [('f1 score',f1_lg,f1_knn,f1_svm),\n",
    "                 ('','','',''),\n",
    "                  ('accuracy %',acc_lg,acc_knn,acc_svm),\n",
    "                 ('','','',''),\n",
    "                 ('confusion matrix',conf_lg[0], conf_knn[0], conf_svm[0]),\n",
    "                ('',conf_lg[1], conf_knn[1], conf_svm[1]),\n",
    "                 ('','','',''),\n",
    "                ('ROC score',roc_c_lg,roc_c_knn,roc_c_svm)]\n",
    "    t = Table(rows=data_rows, names=('metric','Logistic regression', 'KNN', 'SVM'))\n",
    "    print(t)\n",
    "    \n",
    "    #Plot ROC curve\n",
    "    print('\\n\\n-----------------------------ROC curves--------------------------------------\\n\\n')\n",
    "    fig, _ = plt.subplots()\n",
    "    fig.suptitle('Comparison of three ROC curves')\n",
    "    fpr,tpr,thresholds=roc_lg\n",
    "    plt.plot([0, 1],[0, 1],'--')\n",
    "    plt.plot(fpr,tpr,label='Logistic regression :'+str(roc_c_lg))\n",
    "    plt.xlabel('false positive')\n",
    "    plt.ylabel('false negative')\n",
    "    fpr,tpr,thresholds=roc_knn\n",
    "    plt.plot(fpr,tpr,label='KNN :'+str(roc_c_knn))\n",
    "    fpr,tpr,thresholds=roc_svm\n",
    "    plt.plot(fpr,tpr,label='SVM :'+str(roc_c_svm))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Maximum metrics\n",
    "    print('-----------------------------Max of metrics--------------------------------------\\n\\n')\n",
    "    data_rows = [('max f1 score',algo_with_max_metric(f1_lg,f1_knn,f1_svm)),\n",
    "                 ('','','',''),\n",
    "                  ('max accuracy %',algo_with_max_metric(acc_lg,acc_knn,acc_svm)),\n",
    "                 ('','','',''),\n",
    "                ('max ROC score',algo_with_max_metric(roc_c_lg,roc_c_knn,roc_c_svm))]\n",
    "    t = Table(rows=data_rows, names=('metric','Learning algorithm winnig'))\n",
    "    print(t)\n",
    "    \n",
    "# Function returning name of winnig algorithm based on a single metric\n",
    "def algo_with_max_metric(a,b,c):\n",
    "    max_metric = max(a,b,c)\n",
    "    if max_metric == a:\n",
    "        return 'Logistic regression'\n",
    "    elif max_metric == b:\n",
    "        return 'KNN'\n",
    "    else:\n",
    "        return 'SVM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c0b0c",
   "metadata": {},
   "source": [
    "## Final Model Comparison and Feature Importance\n",
    "\n",
    "- Compare Logistic Regression vs KNN vs SVM.\n",
    "- Use accuracy, F1-score, ROC-AUC, and confusion matrices.\n",
    "- Extract feature importance using SVM coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c53f7",
   "metadata": {},
   "source": [
    "### Contribution (Aditya Saxena)\n",
    "- Supported feature engineering and data preparation tasks.\n",
    "- Contributed to algorithm implementation and parameter tuning.\n",
    "- Helped in validation and performance comparison of outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355c834",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "- Synthesizes model evaluation across multiple metrics rather than relying only on accuracy.\n",
    "- Confirms the Linear SVM as the best-performing model in both discrimination power and balance.\n",
    "- Enables feature importance extraction for actionable educational insights and early intervention support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lg_knn_svm(yt_knn,yp_knn,yt_lg,yp_lg,yt_svm,yp_svm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
