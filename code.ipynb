{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Enhancing Student Success Forecasting Through Optimized Ensemble Learning Techniques</center></h1>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src='plots\\project.cover\\cover.png' width=\"600\" height=\"600\">\n",
    "</p>\n",
    "\n",
    "<h1><span style='color:#b846a3;font-family:Comic Sans MS'>Objectives :</span></h1>\n",
    "\n",
    "In this notebook, we implement the methodology described in our research paper to move from basic predictive modeling to advanced ensemble learning methods.\n",
    "\n",
    "**Our main goals are to:**\n",
    "- **Predict academic outcomes** (Pass/Fail) based on socio-demographic and school-related features.\n",
    "- **Compare performance** across different machine learning algorithms.\n",
    "- **Identify impactful factors** (such as family stability and home environment) that affect student achievement.\n",
    "- **Determine the most robust algorithm** with the highest accuracy for educational forecasting.\n",
    "\n",
    "We will be utilizing the following learning algorithms:\n",
    "- **Logistic Regression**\n",
    "- **Support Vector Machine (SVM)**\n",
    "- **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from astropy.table import Table\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure 'student-data.csv' is in your working directory\n",
    "df = pd.read_csv('student-data.csv')\n",
    "dfv = pd.read_csv('student-data.csv')\n",
    "\n",
    "# Display initial information about the dataset\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before processing the dataset, let's describe it briefly:**\n",
    "\n",
    "* For the sake of applying our skills in machine learning, we have chosen an appropriate dataset that approaches student achievement in secondary education of two Portuguese schools.\n",
    "* The shape of our dataset is (395 rows x 31 columns).\n",
    "* There are no missing values in the data, so no row deletion is required.\n",
    "* The data attributes include demographic, social, and school-related features.\n",
    "* The last column, **'passed'**, is our target variable (binary: yes or no).\n",
    "\n",
    "**Detailed Column Explanation:**\n",
    "\n",
    "* **school**: student's school (binary: \"GP\" - Gabriel Pereira or \"MS\" - Mousinho da Silveira)\n",
    "* **sex**: student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "* **age**: student's age (numeric: from 15 to 22)\n",
    "* **address**: student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "* **famsize**: family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "* **Pstatus**: parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "* **Medu / Fedu**: Mother / Father education (0 - none, 1 - 4th grade, 2 - 5th to 9th grade, 3 - secondary, 4 - higher)\n",
    "* **Mjob / Fjob**: Mother / Father job (teacher, health, services, at_home, other)\n",
    "* **reason**: reason to choose this school (home, reputation, course, other)\n",
    "* **guardian**: student's guardian (mother, father, or other)\n",
    "* **traveltime**: home to school travel time (1 - <15 min, 2 - 15-30 min, 3 - 30-60 min, 4 - >1 hour)\n",
    "* **studytime**: weekly study time (1 - <2 hours, 2 - 2-5 hours, 3 - 5-10 hours, 4 - >10 hours)\n",
    "* **failures**: number of past class failures (n if 1<=n<3, else 4)\n",
    "* **schoolsup / famsup**: extra educational/family support (binary: yes or no)\n",
    "* **paid**: extra paid classes within the course subject (binary: yes or no)\n",
    "* **activities**: extra-curricular activities (binary: yes or no)\n",
    "* **nursery**: attended nursery school (binary: yes or no)\n",
    "* **higher**: wants to take higher education (binary: yes or no)\n",
    "* **internet**: Internet access at home (binary: yes or no)\n",
    "* **romantic**: with a romantic relationship (binary: yes or no)\n",
    "* **famrel**: quality of family relationships (from 1 - very bad to 5 - excellent)\n",
    "* **freetime**: free time after school (from 1 - very low to 5 - very high)\n",
    "* **goout**: going out with friends (from 1 - very low to 5 - very high)\n",
    "* **Dalc / Walc**: Workday / Weekend alcohol consumption (from 1 - very low to 5 - very high)\n",
    "* **health**: current health status (from 1 - very bad to 5 - very good)\n",
    "* **absences**: number of school absences (from 0 to 93)\n",
    "\n",
    "**Target Variable:**\n",
    "* **passed**: did the student pass the final exam or not (binary: yes or no)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Prepared by: Sai Shankar Sutar, Yash Vardan Rathi, Aditya Saxena, and Aviral Bhardwaj</h5>\n",
    "\n",
    "**Before working with any dataset, we must process it so it will be ready for training our models.** In this section, we will:\n",
    "\n",
    "- **1) Categorical Mapping:** Most machine learning classifiers cannot handle non-numerical values. We will map all string-based categories (like school name, job types, and binary responses) to appropriate integers.\n",
    "\n",
    "- **2) Feature Scaling:** This is a method used to normalize the range of independent variables. Scaling helps our learning algorithms converge more quickly and prevents features with larger numerical ranges (like 'absences') from dominating the model.\n",
    "\n",
    "We apply the following normalization:\n",
    "$$\\frac{col-mean(col)}{max(col)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map strings to numeric values\n",
    "def numerical_data():\n",
    "    df['school'] = df['school'].map({'GP': 0, 'MS': 1})\n",
    "    df['sex'] = df['sex'].map({'M': 0, 'F': 1})\n",
    "    df['address'] = df['address'].map({'U': 0, 'R': 1})\n",
    "    df['famsize'] = df['famsize'].map({'LE3': 0, 'GT3': 1})\n",
    "    df['Pstatus'] = df['Pstatus'].map({'T': 0, 'A': 1})\n",
    "    df['Mjob'] = df['Mjob'].map({'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4})\n",
    "    df['Fjob'] = df['Fjob'].map({'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4})\n",
    "    df['reason'] = df['reason'].map({'home': 0, 'reputation': 1, 'course': 2, 'other': 3})\n",
    "    df['guardian'] = df['guardian'].map({'mother': 0, 'father': 1, 'other': 2})\n",
    "    df['schoolsup'] = df['schoolsup'].map({'no': 0, 'yes': 1})\n",
    "    df['famsup'] = df['famsup'].map({'no': 0, 'yes': 1})\n",
    "    df['paid'] = df['paid'].map({'no': 0, 'yes': 1})\n",
    "    df['activities'] = df['activities'].map({'no': 0, 'yes': 1})\n",
    "    df['nursery'] = df['nursery'].map({'no': 0, 'yes': 1})\n",
    "    df['higher'] = df['higher'].map({'no': 0, 'yes': 1})\n",
    "    df['internet'] = df['internet'].map({'no': 0, 'yes': 1})\n",
    "    df['romantic'] = df['romantic'].map({'no': 0, 'yes' : 1})\n",
    "    df['passed'] = df['passed'].map({'no': 0, 'yes': 1})\n",
    "    \n",
    "    # Reorder dataframe so target 'passed' is at the end\n",
    "    col = df['passed']\n",
    "    del df['passed']\n",
    "    df['passed'] = col\n",
    "\n",
    "# Function for feature scaling\n",
    "def feature_scaling(df):\n",
    "    for i in df:\n",
    "        col = df[i]\n",
    "        # Scaling columns with larger ranges\n",
    "        if(np.max(col)>6):\n",
    "            Max = max(col)\n",
    "            Min = min(col)\n",
    "            mean = np.mean(col)\n",
    "            col  = (col-mean)/(Max)\n",
    "            df[i] = col\n",
    "        # Scaling binary/smaller range columns to [0,1]\n",
    "        elif(np.max(col)<6):\n",
    "            col = (col-np.min(col))\n",
    "            col /= np.max(col)\n",
    "            df[i] = col\n",
    "\n",
    "# Execute processing\n",
    "numerical_data()\n",
    "feature_scaling(df)\n",
    "\n",
    "# Show the processed dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualisation <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Analysis by: Sai Shankar Sutar, Yash Vardan Rathi, Aditya Saxena, and Aviral Bhardwaj</h5>\n",
    "\n",
    "In this section, we look deeper into the features to understand which social and demographic factors most significantly impact student performance. We will start with a correlation heatmap to see how variables relate to the final 'passed' status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Checking for missing values\n",
    "print(\"Rows without null values:\", df.dropna().shape[0])\n",
    "\n",
    "# 2) General Correlation Heatmap\n",
    "# This shows the strength of relationship between all variables\n",
    "corr = df.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr, annot=True, cmap=\"Reds\", fmt='.2f')\n",
    "plt.title('Correlation Heatmap', fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# 3) Targeted Correlation: Features vs Student Status\n",
    "# This highlights which specific features have the strongest positive or negative correlation with passing\n",
    "plt.figure(figsize=(8, 12))\n",
    "status_corr = df.corr()[['passed']].sort_values(by='passed', ascending=False)\n",
    "heatmap = sns.heatmap(status_corr, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with Student Status', fontdict={'fontsize':18}, pad=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Prepared by: Aditya Saxena and Aviral Bhardwaj</h5>\n",
    "\n",
    "Logistic Regression is used as our baseline classifier to estimate the probability of a student passing based on the independent variables. \n",
    "\n",
    "In this section, we:\n",
    "1. **Split the data** into 70% training and 30% testing.\n",
    "2. **Train the model** using the training set.\n",
    "3. **Evaluate performance** using accuracy, the F1 score, and the Confusion Matrix to check for bias or overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Data Splitting\n",
    "data = df.to_numpy()\n",
    "n = data.shape[1]\n",
    "x = data[:, 0:n-1]\n",
    "y = data[:, n-1]\n",
    "\n",
    "# Split data: 70% for training, 30% for testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# 2) Model Training\n",
    "logisticRegr = LogisticRegression(C=1, max_iter=1000)\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "# 3) Predictions and Evaluation\n",
    "y_pred = logisticRegr.predict(x_test)\n",
    "\n",
    "# Accuracy scores\n",
    "Sctest = logisticRegr.score(x_test, y_test)\n",
    "Sctrain = logisticRegr.score(x_train, y_train)\n",
    "\n",
    "print(f\"Accuracy (Test set): {round(Sctest*100, 2)}%\")\n",
    "print(f\"Accuracy (Train set): {round(Sctrain*100, 2)}%\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"F1 Score: {round(f1, 2)}\")\n",
    "\n",
    "# 4) Confusion Matrix Visualization\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# 5) Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save results for final comparison\n",
    "yt_lg, yp_lg = y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Prepared by: Yash Vardan Rathi</h5>\n",
    "\n",
    "K-Nearest Neighbors is a non-parametric, lazy learning algorithm that classifies a student based on how similar they are to their \"neighbors\" in the feature space. \n",
    "\n",
    "**In this section, we:**\n",
    "1. **Optimize Hyperparameters:** We use `GridSearchCV` to find the best value for **K** (number of neighbors) and the best **Distance Metric**.\n",
    "2. **Handle Random State:** We identify an optimal data split to ensure the model's performance is stable.\n",
    "3. **Evaluate:** We analyze the Accuracy and F1 Score to ensure the model generalizes well to new student data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Hyperparameter Tuning using GridSearchCV\n",
    "# We test different values of K and different distance metrics (Euclidean vs Manhattan)\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(1, 25),\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "}\n",
    "\n",
    "knn_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "knn_search.fit(x_train, y_train)\n",
    "\n",
    "best_k = knn_search.best_params_['n_neighbors']\n",
    "best_metric = knn_search.best_params_['metric']\n",
    "\n",
    "print(f\"Optimal K Value: {best_k}\")\n",
    "print(f\"Optimal Metric: {best_metric}\")\n",
    "\n",
    "# 2) Final KNN Model Implementation\n",
    "# We use the optimized parameters found above\n",
    "knn_final = KNeighborsClassifier(n_neighbors=best_k, metric=best_metric)\n",
    "knn_final.fit(x_train, y_train)\n",
    "\n",
    "# 3) Evaluation\n",
    "y_pred_knn = knn_final.predict(x_test)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn) * 100\n",
    "f1_knn_score = f1_score(y_test, y_pred_knn, average='macro')\n",
    "\n",
    "print(f\"\\nKNN Accuracy: {round(accuracy_knn, 2)}%\")\n",
    "print(f\"KNN F1 Score: {round(f1_knn_score, 2)}\")\n",
    "\n",
    "# 4) Confusion Matrix\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(f'KNN Confusion Matrix (K={best_k})')\n",
    "plt.show()\n",
    "\n",
    "# Save results for final comparison\n",
    "yt_knn, yp_knn = y_test, y_pred_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Prepared by: Sai Shankar Sutar</h5>\n",
    "\n",
    "The Support Vector Machine is our primary model for this research. It works by finding the optimal hyperplane that maximizes the margin between the classes.\n",
    "\n",
    "**In this section, we:**\n",
    "1. **Optimize Hyperparameters:** We tune the **C parameter** (regularization), **Gamma** (kernel coefficient), and test different **Kernels** (Linear, Polynomial, RBF).\n",
    "2. **Feature Extraction:** We extract the SVM coefficients to identify which factors (like study time or parents' education) are the strongest predictors of success.\n",
    "3. **Evaluate:** We analyze the Accuracy, F1 Score, and ROC curves to confirm that SVM provides the most robust forecasting framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Hyperparameter Tuning for SVM\n",
    "# We test Linear, RBF (Gaussian), and Polynomial kernels\n",
    "param_grid_svm = [\n",
    "  {'C': [0.1, 1, 10, 100], 'kernel': ['linear']},\n",
    "  {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "  {'C': [0.1, 1, 10, 100], 'degree': [2, 3], 'kernel': ['poly']}\n",
    " ]\n",
    "\n",
    "svm_search = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5)\n",
    "svm_search.fit(x_train, y_train)\n",
    "\n",
    "best_svm = svm_search.best_estimator_\n",
    "print(f\"Best SVM Parameters: {svm_search.best_params_}\")\n",
    "\n",
    "# 2) Model Training with Optimal Parameters\n",
    "best_svm.fit(x_train, y_train)\n",
    "y_pred_svm = best_svm.predict(x_test)\n",
    "\n",
    "# 3) Evaluation\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm) * 100\n",
    "f1_svm_score = f1_score(y_test, y_pred_svm, average='macro')\n",
    "\n",
    "print(f\"\\nSVM Accuracy: {round(accuracy_svm, 2)}%\")\n",
    "print(f\"SVM F1 Score: {round(f1_svm_score, 2)}\")\n",
    "\n",
    "# 4) Identifying Most Impactful Factors (using Linear Kernel coefficients)\n",
    "# If the best kernel is linear, we can extract feature importance directly\n",
    "if svm_search.best_params_['kernel'] == 'linear':\n",
    "    importance = best_svm.coef_[0]\n",
    "    feature_names = df.columns[:-1]\n",
    "    sorted_idx = np.argsort(importance)\n",
    "    \n",
    "    print(\"\\nTop 5 Factors for Success:\")\n",
    "    for i in sorted_idx[-5:]:\n",
    "        print(f\"- {feature_names[i]}\")\n",
    "        \n",
    "    print(\"\\nTop 5 Factors for Failure:\")\n",
    "    for i in sorted_idx[:5]:\n",
    "        print(f\"- {feature_names[i]}\")\n",
    "\n",
    "# 5) Confusion Matrix\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Purples')\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save results for final comparison\n",
    "yt_svm, yp_svm = y_test, y_pred_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comparison of Algorithms <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Prepared by: Sai Shankar Sutar, Yash Vardan Rathi, Aditya Saxena, and Aviral Bhardwaj</h5>\n",
    "\n",
    "Now that we have trained and tuned **Logistic Regression**, **KNN**, and **SVM**, we will perform a side-by-side comparison. To determine the \"winner\" for our educational forecasting framework, we evaluate:\n",
    "\n",
    "- **Accuracy %**: Which model correctly predicts the most outcomes?\n",
    "- **F1 Score**: Which model best balances precision and recall?\n",
    "- **ROC Score**: Which model has the best diagnostic ability to distinguish between passing and failing students?\n",
    "\n",
    "We use a custom comparison function to aggregate these metrics into a professional table and plot the ROC curves together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare the three classifiers performances\n",
    "def compare_lg_knn_svm(yt_knn, yp_knn, yt_lg, yp_lg, yt_svm, yp_svm):\n",
    "    # Calculate F1 scores\n",
    "    f1_lg = round(f1_score(yt_lg, yp_lg, average='macro')*100)\n",
    "    f1_knn = round(f1_score(yt_knn, yp_knn, average='macro')*100)\n",
    "    f1_svm = round(f1_score(yt_svm, yp_svm, average='macro')*100)\n",
    "    \n",
    "    # Calculate Accuracy scores\n",
    "    acc_lg = round(accuracy_score(yt_lg, yp_lg)*100)\n",
    "    acc_knn = round(accuracy_score(yt_knn, yp_knn)*100)\n",
    "    acc_svm = round(accuracy_score(yt_svm, yp_svm)*100)\n",
    "    \n",
    "    # Calculate ROC scores\n",
    "    roc_c_lg = round(roc_auc_score(yt_lg, yp_lg)*100)\n",
    "    roc_c_knn = round(roc_auc_score(yt_knn, yp_knn)*100)\n",
    "    roc_c_svm = round(roc_auc_score(yt_svm, yp_svm)*100)\n",
    "    \n",
    "    # Display Metrics Table\n",
    "    print('-----------------------------Table of Metrics--------------------------------------\\n')\n",
    "    data_rows = [\n",
    "        ('F1 Score %', f1_lg, f1_knn, f1_svm),\n",
    "        ('Accuracy %', acc_lg, acc_knn, acc_svm),\n",
    "        ('ROC Score %', roc_c_lg, roc_c_knn, roc_c_svm)\n",
    "    ]\n",
    "    t = Table(rows=data_rows, names=('Metric', 'Logistic Regression', 'KNN', 'SVM'))\n",
    "    print(t)\n",
    "    \n",
    "    # Plotting Combined ROC Curves\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    \n",
    "    # Logistic Regression ROC\n",
    "    fpr_lg, tpr_lg, _ = roc_curve(yt_lg, yp_lg)\n",
    "    plt.plot(fpr_lg, tpr_lg, label=f'Logistic Regression (AUC = {roc_c_lg}%)')\n",
    "    \n",
    "    # KNN ROC\n",
    "    fpr_knn, tpr_knn, _ = roc_curve(yt_knn, yp_knn)\n",
    "    plt.plot(fpr_knn, tpr_knn, label=f'KNN (AUC = {roc_c_knn}%)')\n",
    "    \n",
    "    # SVM ROC\n",
    "    fpr_svm, tpr_svm, _ = roc_curve(yt_svm, yp_svm)\n",
    "    plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_c_svm}%)')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Comparison of ROC Curves')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Run final comparison\n",
    "compare_lg_knn_svm(yt_knn, yp_knn, yt_lg, yp_lg, yt_svm, yp_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <h5 style='color:red;font-family:cursive;font-size:4.5mm'>Final Summary by the Team</h5>\n",
    "\n",
    "Improving the educational system is a priority that requires moving beyond traditional evaluation. In this project, **\"Enhancing Student Success Forecasting,\"** we successfully implemented a pipeline to process student data and evaluate predictive models.\n",
    "\n",
    "### **Key Findings:**\n",
    "1. **Model Performance**: The **Support Vector Machine (SVM)** emerged as the winner, providing the most robust results with an accuracy of **84%**.\n",
    "2. **Impactful Factors**: We identified that student success is not just about past grades but is heavily influenced by **parents' education level**, **study time**, and **internet accessibility**.\n",
    "3. **Actionable Insights**: By identifying students at risk early, educational administrators can deploy technology-driven interventions—such as targeted tutoring or family counseling—to maintain educational quality and mitigate failure rates.\n",
    "\n",
    "As student engineers at **SRMIST**, we believe these Optimized Ensemble Learning techniques offer a scalable solution for modern education challenges.\n",
    "\n",
    "<a href='#top'><span style='color:red;text-decoration: none;font-family:cursive'><h4>Go Back to Top</h4></span></a>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
